{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set the font size of plots\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read corpus\n",
    "We are going to use a different corpus. This corpus is already labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = [ '..data/sentiment_data_ARA_pos.txt', '..data/sentiment_data_ARA_neg.txt', '..data/sentiment_data_TUN_pos.txt', '..data/sentiment_data_TUN_neg.txt' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    print('Reading file ' + filename + \"...\")\n",
    "    with open(filename, \"r\", encoding='utf8') as textfile:\n",
    "        L = []\n",
    "        for line in textfile:\n",
    "            L.append(line.strip())\n",
    "        print('File contains ', len(L), \"lines.\\n\")\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ara_corpus_pos = read_text_file(corpus_files[0])\n",
    "ara_corpus_neg = read_text_file(corpus_files[1])\n",
    "tun_corpus_pos = read_text_file(corpus_files[2])\n",
    "tun_corpus_neg = read_text_file(corpus_files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify corpus\n",
    "\n",
    "type(ara_corpus_pos),type(ara_corpus_neg),type(tun_corpus_pos),type(tun_corpus_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ara_corpus_pos),len(ara_corpus_neg),len(tun_corpus_pos),len(tun_corpus_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ara_corpus_pos[0])\n",
    "print(ara_corpus_neg[0])\n",
    "print(tun_corpus_pos[0])\n",
    "print(tun_corpus_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine pos and neg corpus into a single corpus for easy manipulation\n",
    "\n",
    "ara_corpus = ara_corpus_pos + ara_corpus_neg\n",
    "ara_corpus_sentiment = len(ara_corpus_pos)*[\"POS\"] + len(ara_corpus_neg)*[\"NEG\"]\n",
    "tun_corpus = tun_corpus_pos + tun_corpus_neg\n",
    "tun_corpus_sentiment = len(tun_corpus_pos)*[\"POS\"] + len(tun_corpus_neg)*[\"NEG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ara_corpus),len(ara_corpus_sentiment),len(tun_corpus),len(tun_corpus_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing & Cleaning\n",
    "\n",
    "We are going to follow the same pipeline of Language classification, except that here the operations should be **adapted to the Arabic language** (instead of the French language).\n",
    "\n",
    "1. Remove useless characters (using ``cleanup_text`` function from TD2)\n",
    "2. Language identification and filtering (using language identification model from TD2)\n",
    "3. Letter normalization\n",
    "4. Tokenization\n",
    "5. Remove stop words\n",
    "6. Word normalization (stemming)\n",
    "7. Remove words that are too short or too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1. Remove useless characters using cleanup_text function\n",
    "import re\n",
    "import html\n",
    "# regexp for word elongation: matches 3 or more repetitions of a word character.\n",
    "two_plus_letters_RE = re.compile(r\"(\\w)\\1{1,}\", re.DOTALL)\n",
    "three_plus_letters_RE = re.compile(r\"(\\w)\\1{2,}\", re.DOTALL)\n",
    "# regexp for repeated words\n",
    "two_plus_words_RE = re.compile(r\"(\\w+\\s+)\\1{1,}\", re.DOTALL)\n",
    "\n",
    "\n",
    "def cleanup_text(text):\n",
    "    \n",
    "    # REMOVE NUMBERS\n",
    "    text = re.sub('[0-9٠-٩]', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "\n",
    "    # Remove user mentions of the form @username\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    \n",
    "    # Replace special html-encoded characters with their ASCII equivalent, for example: &#39 ==> '\n",
    "    if re.search(\"&#\",text):\n",
    "        text = html.unescape(text)\n",
    "\n",
    "    # Remove special useless characters such as _x000D_\n",
    "    text = re.sub(r'_[xX]000[dD]_', '', text)\n",
    "\n",
    "    # Replace all non-word characters (such as emoticons, punctuation, end of line characters, etc.) with a space\n",
    "    text = re.sub('[\\W_]', ' ', text)\n",
    "\n",
    "    # Remove redundant white spaces\n",
    "    text = text.strip()\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "\n",
    "    # normalize word elongations (characters repeated more than twice)\n",
    "    text = two_plus_letters_RE.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # remove repeated words\n",
    "    text = two_plus_words_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def cleanup(text):\n",
    "    \n",
    "    #Remove all documents that contain a large fraction of latin characters (for example more than 80%)\n",
    "    text = [doc for doc in text if (len(doc)>0 and len(re.findall('[a-zA-Z]',doc))/len(doc)<0.8)]\n",
    "    \n",
    "    #Remove very short documents\n",
    "    text = [doc for doc in text if len(doc)>=10]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this function to each document in the corpus\n",
    "ara_corpus_clean = []\n",
    "tun_corpus_clean = []\n",
    "for doc in ara_corpus:\n",
    "    ara_corpus_clean.append(cleanup_text(doc))\n",
    "for doc in tun_corpus:\n",
    "    tun_corpus_clean.append(cleanup_text(doc))\n",
    "\n",
    "ara_corpus_clean = cleanup(ara_corpus_clean)\n",
    "tun_corpus_clean = cleanup(tun_corpus_clean)\n",
    "print(len(tun_corpus_clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2. Language identification and filtering\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "# #import the model\n",
    "# loaded_model = pickle.load(open('C:\\\\Users\\\\21620\\\\TextMiningProject\\\\Language detection\\\\lang_model.sav', 'rb'))\n",
    "\n",
    "# #import the bow model\n",
    "# bow_model = pickle.load(open('C:\\\\Users\\\\21620\\\\TextMiningProject\\\\Language detection\\\\bow_model.sav', 'rb'))\n",
    "\n",
    "#classification function\n",
    "# def predict_lang(text):\n",
    "#     inst=[]\n",
    "#     inst.append(text)\n",
    "#     text_vect=bow_model.transform(inst)\n",
    "#     prob=loaded_model.predict_proba(text_vect)[0]\n",
    "#     x=prob[0]\n",
    "#     #print(x,prob[1])\n",
    "#     if x <0.7 and x>0.3:\n",
    "#         return(\"OTHER\")\n",
    "#     else:\n",
    "#         return loaded_model.predict(text_vect)[0]\n",
    "    \n",
    "#test\n",
    "#print(predict_lang(\"مجموعة من الأغاني التي قدمها طارق العربي طرقان و أبناؤه محمد العربي و ديمة و تالة في برنامج صاحبة السعادة\"))\n",
    "ara_corpus_filtered = []\n",
    "tun_corpus_filtered = []\n",
    "ara_corpus_filtered = ara_corpus_clean\n",
    "tun_corpus_filtered = tun_corpus_clean\n",
    "#for doc in ara_corpus_clean:\n",
    " #   if (predict_lang(doc)==\"ARA\"):\n",
    "  #      ara_corpus_filtered.append(doc)\n",
    "#for doc in tun_corpus_clean:\n",
    "    #if (predict_lang(doc)==\"TUN\"):\n",
    "        #tun_corpus_filtered.append(doc)\n",
    "print(len(tun_corpus_filtered),len(ara_corpus_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ara_corpus_filtered[0])\n",
    "print(tun_corpus_filtered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3. Letter normalization\n",
    "# Hint: which Arabic letters are equivalent n social media text?  e.g. alef, tah marbuta, dhad and dhad toushel, etc.\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    alif='[ﺀٱءآإأ]'\n",
    "    baa='[ﺒپ]'\n",
    "    laa='[ﻵﻹﻷ]'\n",
    "    kaf='[کگ]'\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    k=[]\n",
    "    for doc in text:\n",
    "        doc = re.sub('ة','ت', doc)\n",
    "        doc = re.sub('ظ','ض' ,doc)\n",
    "        doc = re.sub(alif,'ا' ,doc)\n",
    "        doc = re.sub(laa,'لا' ,doc)\n",
    "        doc = re.sub(baa,'ب' ,doc)\n",
    "        doc = re.sub(kaf,'ك' ,doc)\n",
    "        doc = re.sub('چ','ج' ,doc)\n",
    "        doc = re.sub('ڤ','ف' ,doc)\n",
    "        doc = re.sub('ڼ','ن', doc)\n",
    "        doc = re.sub('ۋ','و', doc)\n",
    "        doc = re.sub('ھ','ه', doc)\n",
    "        doc = re.sub('ژ','ر', doc)\n",
    "        doc = re.sub('ڜ','ش', doc)\n",
    "        doc = re.sub(\"ى\", \"ي\", doc)\n",
    "        doc = re.sub(\"ئ\", \"ا\", doc)\n",
    "        doc = re.sub('ؤ','ا', doc)\n",
    "        doc = re.sub(noise, '', doc)  #remove short vowels and other symbols \n",
    "        k.append(doc)\n",
    "\n",
    "    text=k\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ara_corpus_filtered = normalize(ara_corpus_filtered)\n",
    "tun_corpus_filtered = normalize(tun_corpus_filtered)\n",
    "print(len(tun_corpus_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##4. Tokenization -- Complete code below (same as in TD1)\n",
    "\n",
    "# COMPLETE THE CODE BELOW\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('[^_\\W]+')\n",
    "ara_corpus_tokenized = [tokenizer.tokenize(doc) for doc in ara_corpus_filtered]\n",
    "tun_corpus_tokenized = [tokenizer.tokenize(doc) for doc in tun_corpus_filtered]\n",
    "print(ara_corpus_tokenized[:5])\n",
    "print(tun_corpus_tokenized[:5])\n",
    "print(len(tun_corpus_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify first document in corpus\n",
    "print(ara_corpus_tokenized[0])\n",
    "print(tun_corpus_tokenized[0])\n",
    "print(len(tun_corpus_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##5. Remove stop words -- based on a 'standard' list of stopwords for the Arabic language.\n",
    "\n",
    "# COMPLETE THE CODE BELOW  (See TD1)\n",
    "\n",
    "# Load stop words from NLTK library\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_ar = stopwords.words('arabic')\n",
    "stop_words_ar = stop_words_ar + ['كان','أصبح','أضحى','أمسى','بات','ظلَّ','صار','من','إلى','حتى','خلا','عدا','في',\n",
    "                               'عن','على','مذ','منذ','ومتى','متى','أنت','أنتما','أنتم','أنتن','إياك','إياكما','إياكن','أنا','نحن','هو','هي','هن','هما']\n",
    "type(stop_words_ar),len(stop_words_ar)\n",
    "print(stop_words_ar[0:10])\n",
    "\n",
    "# FEEL FREE TO ADD MORE WORDS TO THIS LIST IF YOU WANT ...\n",
    "\n",
    "\n",
    "# For each document, remove stop words\n",
    "ara_corpus_tokenized = [[word for word in doc  if word not in stop_words_ar] for doc in ara_corpus_tokenized]\n",
    "tun_corpus_tokenized = [[word for word in doc  if word not in stop_words_ar] for doc in tun_corpus_tokenized]\n",
    "len(tun_corpus_tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##6. Stemming\n",
    "# Hints: stemming is a difficult task for the Arabic language because words are often combined into one word (called agglutination).\n",
    "#     You should first visually inspect all the words in your corpus to get an idea about which words are good candidates for stemming ...\n",
    "#     Then try to think of a few simple stemming heuristics (regular expressions), such as: remove certain prefixes (e.g. al), remove certain suffixes (e.g. 'ouna') ...\n",
    "# SKIP THIS STEP IN CLASS TO SAVE TIME. COMPLETE IT AT HOME.\n",
    "from snowballstemmer import stemmer\n",
    "ara_corpus_doc= []\n",
    "ara_corpus_stemmed=[]\n",
    "tun_corpus_doc= []\n",
    "tun_corpus_stemmed= []\n",
    "ar_stemmer = stemmer(\"arabic\")\n",
    "for doc in ara_corpus_tokenized:\n",
    "    for word in doc:\n",
    "        ara_corpus_doc.append(ar_stemmer.stemWord(word))\n",
    "    ara_corpus_doc= []\n",
    "    ara_corpus_stemmed.append(ara_corpus_doc)\n",
    "    \n",
    "for doc in tun_corpus_tokenized:\n",
    "    for word in doc:\n",
    "        tun_corpus_doc.append(ar_stemmer.stemWord(word))\n",
    "    tun_corpus_doc= []\n",
    "    tun_corpus_stemmed.append(tun_corpus_doc)\n",
    "print(len(ara_corpus_stemmed),len(tun_corpus_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the corpus for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, concatenate the words in the cleaned corpus (because BOW method in scikit-learn requires this format)\n",
    "ara_corpus_bow = [' '.join(doc) for doc in ara_corpus_stemmed]\n",
    "tun_corpus_bow = [' '.join(doc) for doc in tun_corpus_stemmed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the vocabulary set\n",
    "Extract the vocabulary set from our corpus and calculate IDF values of each word in this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters of the BOW model\n",
    "# FEEL FREE TO MODIFY THESE PARAMETERS AS NEEDED ...\n",
    "max_words = 10000\n",
    "maxdf = 1.0\n",
    "mindf = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of this class\n",
    "bow_model_ara = TfidfVectorizer(max_df=maxdf, min_df=mindf, stop_words=[], use_idf = True)\n",
    "bow_model_tun = TfidfVectorizer(max_df=maxdf, min_df=mindf, stop_words=[], use_idf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call fit() method in order to prepare BOW method (determine vocabulary and IDF values)\n",
    "bow_model_ara.fit(ara_corpus_bow)\n",
    "bow_model_tun.fit(tun_corpus_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the DTM matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the transform method in order to calculate DTM matrix of our corpus\n",
    "ara_bow_dtm = bow_model_ara.transform(ara_corpus_bow)\n",
    "tun_bow_dtm = bow_model_tun.transform(tun_corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the type and size of this matrix\n",
    "print(type(ara_bow_dtm))\n",
    "print(ara_bow_dtm.shape)\n",
    "\n",
    "print(type(tun_bow_dtm))\n",
    "print(tun_bow_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visually inspect the vocabulary\n",
    "This should help you **tune** the BOW configuration parameters (i.e. min_df, max_df, etc.) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vocabulary of BOW -- i.e. the words that were selected by BOW method to be in the vocabulary\n",
    "bow_vocab_ara = bow_model_ara.get_feature_names()\n",
    "print(type(bow_vocab_ara), len(bow_vocab_ara))\n",
    "bow_vocab_tun = bow_model_tun.get_feature_names()\n",
    "print(type(bow_vocab_tun), len(bow_vocab_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words that were ignored (and were not included in the vocabulary)\n",
    "ignored_words_ara = bow_model_ara.stop_words_\n",
    "print(type(ignored_words_ara),len(ignored_words_ara))\n",
    "ignored_words_tun = bow_model_tun.stop_words_\n",
    "print(type(ignored_words_tun),len(ignored_words_tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DON'T DO THIS !!! THERE ARE TOO MANY IGNORED WORDS\n",
    "#print(ignored_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put vocavulary and their IDF values in a data frame\n",
    "df_ara = pd.DataFrame(dict(Word=bow_vocab_ara,IDF=bow_model_ara.idf_))\n",
    "df_tun = pd.DataFrame(dict(Word=bow_vocab_tun,IDF=bow_model_tun.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show vocabulary words that have SMALLEST IDF values (i.e. that have the largest document frequencies)\n",
    "df_ara.sort_values(\"IDF\", inplace=False, ascending = True).head(10)\n",
    "print(df_ara['Word'])\n",
    "df_tun.sort_values(\"IDF\", inplace=False, ascending = True).head(10)\n",
    "print(df_tun['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show vocabulary words that have LARGEST IDF values (i.e. that have the smallest document frequencies)\n",
    "df_ara.sort_values(\"IDF\", inplace=False, ascending = False).head(10)\n",
    "df_tun.sort_values(\"IDF\", inplace=False, ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can save the vocabulary into a file\n",
    "# df.sort_values(\"IDF\", inplace=False, ascending = True).to_csv(\"./models/bow_vocab.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove documents that do not contain any vocabulary terms\n",
    "i.e. remove rows in the DTM that are all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_terms_per_doc = np.array((ara_bow_dtm>0).sum(axis=1)) \n",
    "nb_terms_per_doc = nb_terms_per_doc.ravel()\n",
    "idx = nb_terms_per_doc>0\n",
    "ara_bow_dtm_filt = ara_bow_dtm[idx,:]\n",
    "ara_corpus_bow_filt = [ara_corpus_bow[i] for i,x in enumerate(idx) if x]\n",
    "ara_corpus_sentiment_filt = [ara_corpus_sentiment[i] for i,x in enumerate(idx) if x]\n",
    "\n",
    "nb_terms_per_doc1 = np.array((tun_bow_dtm>0).sum(axis=1)) \n",
    "nb_terms_per_doc1 = nb_terms_per_doc1.ravel()\n",
    "idx1 = nb_terms_per_doc1>0\n",
    "tun_bow_dtm_filt = tun_bow_dtm[idx1,:]\n",
    "tun_corpus_bow_filt = [tun_corpus_bow[i] for i,x in enumerate(idx1) if x]\n",
    "tun_corpus_sentiment_filt = [tun_corpus_sentiment[i] for i,x in enumerate(idx1) if x]\n",
    "#print(len(tun_corpus_sentiment),len(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ara = ara_bow_dtm_filt\n",
    "y_ara = ara_corpus_sentiment_filt\n",
    "X_tun = tun_bow_dtm_filt\n",
    "y_tun = tun_corpus_sentiment_filt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "\n",
    "X_train_ara, X_test_ara, y_train_ara, y_test_ara = train_test_split(X_ara, y_ara, test_size = 0.3, random_state=1996)\n",
    "X_train_tun, X_test_tun, y_train_tun, y_test_tun = train_test_split(X_tun, y_tun, test_size = 0.3, random_state=1996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using Logistic Regression method\n",
    "\n",
    "LR_model_ara = LogisticRegression(penalty='l2')\n",
    "LR_model_ara.fit(X_train_ara, y_train_ara)\n",
    "print(X_train_ara)\n",
    "print(y_train_ara)\n",
    "\n",
    "\n",
    "LR_model_tun = LogisticRegression(penalty='l2')\n",
    "LR_model_tun.fit(X_train_tun, y_train_tun)\n",
    "print(X_train_tun)\n",
    "print(y_train_tun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this model to predict the sentiment category of test documents\n",
    "y_pred_LR_ara = LR_model_ara.predict(X_test_ara)\n",
    "y_pred_LR_tun = LR_model_tun.predict(X_test_tun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_pred_LR_ara),len(y_pred_LR_ara)\n",
    "type(y_pred_LR_tun),len(y_pred_LR_tun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the classification rate of this classifier\n",
    "print(metrics.accuracy_score(y_test_ara, y_pred_LR_ara),metrics.accuracy_score(y_test_tun, y_pred_LR_tun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- The logistic regression model has one parameter per feature (i.e. vocabulary word).\n",
    "- Most positive values indicate parameters that contribute most to class 1\n",
    "- Most negative values indicate parameters contribute most to class -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the model using ALL DATA\n",
    "LR_model2_ara = LogisticRegression(penalty='l2')\n",
    "LR_model2_ara.fit(X_ara, y_ara)\n",
    "LR_model2_tun = LogisticRegression(penalty='l2')\n",
    "LR_model2_tun.fit(X_tun, y_tun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the coefficients (parameter) of the LR model\n",
    "LR_coefs_ara = LR_model2_ara.coef_   #2D array with only one row\n",
    "LR_coefs_ara = LR_coefs_ara.ravel()  #convert to a 1D array\n",
    "LR_coefs_tun = LR_model2_tun.coef_   #2D array with only one row\n",
    "LR_coefs_tun = LR_coefs_tun.ravel()  #convert to a 1D array\n",
    "print(type(LR_coefs_ara),type(LR_coefs_tun))\n",
    "print(LR_coefs_ara.shape,LR_coefs_tun.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Naive Bayes classification model\n",
    "\n",
    "NB_model_ara = MultinomialNB(alpha = 1.0)\n",
    "NB_model_ara.fit(X_train_ara, y_train_ara)\n",
    "print(X_train_ara)\n",
    "print(y_train_ara)\n",
    "\n",
    "\n",
    "NB_model_tun = MultinomialNB(alpha = 1.0)\n",
    "NB_model_tun.fit(X_train_tun, y_train_tun)\n",
    "print(X_train_tun)\n",
    "print(y_train_tun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read documentation\n",
    "# ?MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this model to predict the sentiment category of test documents\n",
    "y_pred_NB_ara = NB_model_ara.predict(X_test_ara)\n",
    "y_pred_NB_tun = NB_model_tun.predict(X_test_tun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification rate\n",
    "print(metrics.accuracy_score(y_test_ara, y_pred_NB_ara),metrics.accuracy_score(y_test_tun, y_pred_NB_tun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename1 = 'sentiment_analysis_model_ara.sav'\n",
    "filename2 = 'sentiment_analysis_model_tun.sav'\n",
    "filename3 = 'bow_model_ara.sav'\n",
    "filename4 = 'bow_model_tun.sav'\n",
    "pickle.dump(NB_model_ara, open(filename1, 'wb'))\n",
    "pickle.dump(NB_model_tun, open(filename2, 'wb'))\n",
    "pickle.dump(bow_model_ara, open(filename3, 'wb'))\n",
    "pickle.dump(bow_model_tun, open(filename4, 'wb'))\n",
    "\n",
    "\n",
    "loaded_model_ara = pickle.load(open('sentiment_analysis_model_ara.sav', 'rb'))\n",
    "loaded_model_tun = pickle.load(open('sentiment_analysis_model_tun.sav', 'rb'))\n",
    "bow_model_ara = pickle.load(open('bow_model_ara.sav', 'rb'))\n",
    "bow_model_tun = pickle.load(open('bow_model_tun.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_ara(text):\n",
    "    inst=[]\n",
    "    inst.append(text)\n",
    "    text_vect=bow_model_ara.transform(inst)\n",
    "    prob=loaded_model_ara.predict_proba(text_vect)[0]\n",
    "    x=prob[0]\n",
    "    print(x)\n",
    "    if x <0.8 and x>0.2:\n",
    "        return(\"neutre\")\n",
    "    else:\n",
    "        return loaded_model_ara.predict(text_vect)[0]\n",
    "\n",
    "def predict_sentiment_tun(text):\n",
    "    inst=[]\n",
    "    inst.append(text)\n",
    "    text_vect=bow_model_tun.transform(inst)\n",
    "    prob=loaded_model_tun.predict_proba(text_vect)[0]\n",
    "    x=prob[0]\n",
    "    print(x)\n",
    "    if x <0.7 and x>0.3:\n",
    "        return(\"neutre\")\n",
    "    else:\n",
    "        return loaded_model_tun.predict(text_vect)[0]\n",
    "    \n",
    "#predict_sentiment_ara(\" لحميد العيب \")\n",
    "predict_sentiment_tun(\"رة عملوها الفقراء والمحتاجين و المظلومين ثورة القضاء عالفقر و البطالة و الاحتياج أما هذا الكل مصارش منو شي ... الثورة فكوها الأحزاب و السياسيين ثورة الشباب فكوها الشياب و العجائز و فكوها السراق و المافيات .. البطالة في إرتفاع متزايد و الفقر في إرتفاع متزايد و حتى الطبقة المتوسطة قضاو علاها ... ثورة فيبالنا باش تحسن أوضاعنا المعيشية فجأة ركبو الشواذ و المثليين و العاهرات و القرودة عالحدث و ولاو تقول علاهم اصحاب حق في الثورة باش يحققو مطالبهم اللاأخلاقية ... ثورة زيادة العهر و زيادة الشواذ و المثليين و القرودة و زيادة البطالة و الفقر و التهميش الخ الخ الخ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
